---
title: "Sissejuhatus Statistikasse"
output: html_notebook
---

```{r}
library(tidyverse)
library(ggthemes)
library(psych)
library(ggjoy)
library(corrgram)
```


#1. Milleks statistika?

Andmeanalüüs ja statistika (siin sünonüümid) on lahutamatu osa igast loodusteadusest. Järgnevalt seletan, miks.

##1.1 Suur ja väike maailm

Kuna maailm on liiga suur ja keeruline, et seda otse uurida, lõikavad teadlased selle väiksemateks tükkideks, kasutades tordilabidana teaduslike hüpoteese. Tüüpiline hüpotees pakub välja mittematemaatilise seletuse mõnele kitsalt piiritletud loodusnähtusele. Näiteks darvinistlik evolutsiooniteooria püüab seletada evolutsiooni toimemehhanisme. Seda teooriat võib võrrelda empiiriliste andmetega. 

Mis juhtub, kui teie lemmikhüpotees on andmetega kooskõlas? Kas see tähendab, et see hüpotees on tõene? Või, et see on tõenäoliselt tõene? Kahjuks on vastus mõlemale küsimusele eitav. Põhjuseks on asjaolu, et enamasti leiab iga nähtuse seletamiseks rohkem kui ühe alternatiivse teadusliku hüpoteesi (näit. lamarksistlik evolutsiooniteooria) ning rohkem kui üks üksteist välistav hüpotees võib olla olemasolevate andmetega võrdses kooskõlas. Asja teeb veel hullemaks, et teoreetiliselt on võimalik sõnastada lõpmatult palju erinevaid teooriaid, mis kõik pakuvad alternatiivseid ja üksteist välistavaid seletusi samale nähtusele. 

Olgu peale, kui me vaatame maailma kõiketeadja jumala perspektiivist, siis tema võib vaadelda kõikehõlmava tõendusmaterjali sobivust kõigi võimalike teooriatega ning valida välja selle ainsa teooria, mis kõige paremini tõendusmaterjaliga sobib. Kuigi, see eeldaks, et jumalal on lõpmata palju andmeid, sest muidu ei oleks tal loogiliselt võimalik lõpmata paljude teooriate vahel valida - aga jumala jaoks on kõik võimalik. Igal juhul meie, surelike, jaoks tähendab see, et teaduslikus "faktis" saab alati kahelda sest kunagi ei või kindel olla, et parimad teooriad lõpmata suurest teooriapilvest ei ole meil täiesti tähelepanuta jäänud ning, et meie jaoks eksisteerivad andmed kajastaksid hästi kõiki võimalikke andmeid! On selge nagu seebivesi, et mida vähem aega me kulutame teoorialoomeks ja andmete kogumiseks, seda vähem usutavad on ka meie teaduslikud järeldused. Enamasti on nii, et mida kehvem on olukord andmerindel, seda rohkem vajame statistikat. Kui meil õnnestuks oma andmetest ilma statistikata saia teha, ei kõhkleks me hetkegi! Eriti, kuna statistikaga käivad käsikäes statistilised mudelid.         

###Mudeli väike maailm

  Ülalmainitud teadusliku meetodi puudused tingivad, et meie huvides on oma teaduslikke probleeme veel ühe taseme võrra lihtsustada, taandades need statistilisteks probleemideks. Selleks tuletame me tavakeelsest ja laiahaardelisest teaduslikust teooriast täpselt formuleeritud matemaatilise mudeli ning seejärel asume uurima oma mudelit. 
  Mudeli eeliseks teooria ees on, et hästi konstrueeritud mudel on lihtsamini mõistetav --- erinevalt vähegi keerulisemast teaduslikust hüpoteesist on mudeli eeldused ja ennustused läbinähtavad ja täpselt formuleeritavad. Mudeli puuduseks on aga, et erinevalt teooriast ei ole mingit võimalust, et mudel vastaks tegelikkusele ehk oleks tõene. Seda sellepärast, et mudel on taotluslikult lihtsustav (erandiks on puhtalt ennustuslikud mudelid, mis on aga enamasti läbinähtamatu struktuuriga). Mudel on kas kasulik või kasutu; teooria on kas tõene või väär. Mudeli ja maailma vahel võib olla kaudne "peegeldus", aga mitte kunagi otsene side. Seega, ükski number, mis arvutatakse mudeli raames, ei kandu sama numbrina üle teaduslikku ega päris maailma. Ja kogu statistika (ka mitteparameetriline) toimub mudeli väikses maailmas. Arvud, mida statistika teile pakub, elavad mudeli maailmas; samas kui teie teaduslik huvi on suunatud päris maailmale. Näiteks 95% usaldusintervall ei tähenda, et te peaksite olema 95% kindel, et tõde asub selles intervallis – sageli ei tohiks te seda nii julgelt tõlgendada isegi kitsas mudeli maailmas. 

####Millest koosneb mudel?

Tüüpiline mudel püüab formaalselt kirjeldada andmeid genereerivat füüsikalist (v bioloogilisi) protsessi. Näiteks kirjeldame me sageli produkti kuhjumist ensüümreaktsioonis eksponentsiaalse funktsioni (mudeli) abil. Kui meie andmed seda tüüpi funktiooniga sobivad, ütleb see meile midagi konkreetse ensüümi töömehhanismi kohta teadusliku teooria raames. Teisest küljest, need mudelid, mis on "generatiivsed", suudavad ka simuleerida uusi andmeid. Sealhulgas ka selliseid, mida päris maailmas ei saa kunagi esineda sest seal puuduvad vastavad tingimused. Mudelisse saab aga sisse kirjutada igasuguseid tingimusi ehk parameetri väärtusi (näit substraadi konsentratsioone, mida me ei suuda "päriselt" saavutada). 

  Mudelid sisaldavad (1) matemaatilisi struktuure, mis on konstantsed ning määravad mudeli üldise struktuuri ning (2) parameetreid, mille väärtused pole mudeli struktuuri poolt ette antud ning mida saab andmete põhjal tuunida. Seda tuunimist nimetatakse mudeli fittimiseks. Mudelit fittides on eesmärk saavutada antud tüüpi mudeli maksimaalne sobivus andmetega. 
  
  Osad mudelite tüübid on vähem paindlikud kui teised (parameetreid tuunides on neil vähem liikumisruumi) --- kuigi sellised mudelid sobituvad halvemini andmetega, võivad need  ikkagi paremini kui mõni paindlikum mudel tuua välja andmete peidetud olemuse. Mudeldamine eeldab, et me usume, et meie andmetes leidub nii müra (mida mudel võiks ignoreerida), kui signaal (mida mudel püüab tabada). Seega on iga mudel kompromiss üle- ja alafittimise vahel. 
  
  Üks kõige jäigemaid mudeleid on sirge, mis tähendab, et sirge mudel on suure tõenäosusega alafittitud. Keera sirget kuipalju tahad, ta sobitub hästi ainult väheste andmekogudega. Ja need vähesed on genereeritud teatud tüüpi lineaarsete protsesside poolt. Sirge on üks kõige paremini tõlgendatavaid mudeleid. Teises äärmuses on polünoomsed mudelid, mis on väga paindlikud, mida on väga raske tõlgendada ja mille puhul on suur mudeli ülefittimise oht. Ülefititud mudel järgib nii täpselt valimiandmeid, et sobitub hästi valimis leiduva juhusliku müraga ning seetõttu sobitub halvasti järgmise valimiga samast populatsioonist (igal valimil on oma juhuslik müra kuid kõigil valimitel on ühine mitte-juhuslik signaal). Üldiselt on nii, et mida rohkem on mudelis tuunitavaid parameetreid, seda paindlikum on see mudel, seda kergem on seda mudelit valimiandmetega sobitada ja seda raskem on seda mudelit tõlgendada. Veelgi enam, alati on võimalik konstrueerida mudel, mis sobitub täiuslikult lõpliku arvu andmepunktidega (selle mudeli parameetrite arv = n). Selline mudel on täpselt sama informatiivne kui toorandmed --- ja täiesti kasutu.
  
  
```{r}
library(modelr)
library(broom)
df <- tibble(x=c(2, 3, 2.2, 5, 7, 8), y=c(8, 5, 9, 10, 11, 10.1))
mod_e1 <- lm(y ~ x, data = df)
mod_e2 <- lm(y ~ poly(x, 2), data = df)
mod_e3 <- lm(y ~ poly(x, 3), data = df)
mod_e4 <- lm(y ~ poly(x, 4), data = df)
mod_e5 <- lm(y ~ poly(x, 5), data = df)

df %>% 
  tidyr::expand(df) %>% 
  gather_predictions(mod_e1, mod_e2, mod_e3, mod_e4, mod_e5) %>% 
  ggplot(aes(x, pred, colour = model)) +
  geom_line() +
  geom_point(aes(x, y), color="black", size=2) 

```
*Joonis: mod_e1 on tavaline sirge võrrand y = a + b1x (2 parameetrit: a ja b1), mod_e2: y= a + b1x + b2x^2 (3 parameetrit) ... mod_e5: y= a + b1x + b2x^2 + b3x^3 + b4x^4 + b5x^5 (6 parameetrit). mod_e5 vastab täpselt andmepunktidele.*

```{r}
AIC(mod_e1, mod_e2, mod_e3, mod_e4, mod_e5)
```
*AIC näitab, et parim mudel on mod_e4. Kas see on ka kõige kasulikum mudel?*
  
  
  Näiteks sirge võrrandi abil y = a + bx on antud mudel, kus y = x on on see struktuur, mis tagab, et mudel on sirge, ning a ja b on parameetrid, mis määravad sirge puutepunkti y teljega ning sirge tõusunurga. (Samas struktuur y = x + x^2 tagab, et mudel y = a + b1x + b2x^2  on alati parabooli kujuline, ning parameetrite a, b1 ja b2 väärtused määravad selle parabooli täpse kuju.) 
  Seega saab teades a ja b väärtusi ning omistades x-le suvalise meid huvitava väärtuse, ennustada y-i keskmist väärtust. Näiteks, olgu y inimeste pikkus ja x nende samade inimeste kaal, ning andmete vastu fititud mudel: pikkus(cm) = 102 + 0.8 * kaal(kg).
Omistades kaalule väärtuse 80 kg, tuleb mudeli poolt ennustatud keskmine pikkus 102 + 0.8 * 80 = 166 cm. Iga kg lisakaalu ennustab mudeli kohaselt 0.8 cm võrra suuremat pikkust.
 
 Hea mudel on (1) võimalikult lihtsa struktuuriga, mille põhjal on võimalik teha teaduslikke järeldusi protsessi kohta, mis genereeris andmeid, mida mudeli valmistamiseks kasutati; (2) sobitub piisavalt hästi andmetega (eriti uute andmetega, mida ei ole kasutatud selle mudeli fittimiseks), et olla relevantne andmeid genereeriva protsessi kirjeldus; (3) genereerib usutavaid simuleeritud andmeid (mis näitab mudeli fittimise tehnilist kvaliteeti).
 
 Sageli fititkse samade andmetega mitu erinevat tüüpi mudelit ja püütakse otsustada, milline neist vastab kõige paremini eeltoodud tingimustele. Näiteks, kui sirge suudab kaalu järgi pikkust ennustada paremini kui parabool, siis on sirge mudel kooskõlas teadusliku hüpoteesiga, mis annaks mehhanismi protsessile, mille käigus kilode lisandumine viiks laias kaaluvahemikus inimeste pikkuse kasvule. See on teadusliku meetodi analoog, kus võrreldakse oma andmeid erinevate alternatiivsete hüpoteesidega, et leida hüpoteesi, millega andmed parimini sobivad. 
  
Eelpool kirjeldatud mudelid on deterministlikud --- nad ei sisalda hinnangut ebakindlusele ennustuse ümber, ehk kui mudel ennustab, et 80 kg inimene on 166 cm pikkune, siis kui suurt kaalust mittesõltuvat pikkuse varieeruvust võime oodata 80 kg-ste inimeste hulgas? Mudeli ennustuse ebakindluse hindamiseks tuleb mudelile lisada veel üks komponent, nimelt veakomponent, mis sageli tuuakse sisse normaaljaotuse kujul. Veakomponent modelleerib üksikute inimeste pikkuste varieeruvust (mitte keskmise pikkuse varieeruvust) igal mõeldaval ja mittemõeldaval kaalul. *Muide, selline veamudel modelleerib igale x-i väärtusele (kaalule) sama suure y-i suunalise varieeruvuse (pikkuste sd). Suurem osa  statistikast käib just läbi selliste jaburavõitu eelduste, mida keegi päriselt tõeks ei pea, aga millega me siiski arvutuste ja mudelite lihtsuse huvides elama peame.*

####Enimkasutatud statistiline mudel on normaaljaotus. 

Oletame, et meil on kolm andmepunkti ning me usume, et need andmed on juhuslikult tõmmatud normaaljaotusest või sellele lähedasest jaotusest. Mudel on siin normaaljaotus ja seda kasutades me sisuliselt deklareerime, et me usume, et kui me oleksime olnud vähem laisad ja 3 mõõtmise asemel sooritanuks 3000, siis need mõõtmised sobituksid piisavalt hästi meie 3 väärtuse peal fititud normaaljaotusega. Seega, omades 3 andmepunkti, me teame juba umbkaudu, millised tulemused me oleksime saanud korjates 3 miljonit andmepunkti. Oma mudeli abil võime ju modelleerida ükskõik kui palju andmepunkte. 

Aga pidage meeles, et selle mudeli fittimiseks kasutame me neid andmeid, mis meil päriselt on --- ja kui meil on ainult 3 andmepunkti, on suhteliselt tõenäoline, et fititud mudel ei kajasta hästi tegelikkust. Aga see-eest oleme oma 3 närust andmepunkti asendanud matemaatiliselt mugava tihedusfunktsiooniga ning suudame (hästi või halvasti) ennustada uusi andmeid.
  
  **Kuna tegemist on simulatsiooniga, teame, et populatsiooni, kust me tõmbame oma kolmese valimi, keskväärtus on 0 ja sd on 1. Me fitime oma valimi andmetega 2 erinevat mudelit: normaaljaotuse ja Studenti t jaotuse. Pane tähele, et meie kahel mudelil on erinevad parameetrid, millele püüame oma valimi andmete põhjal väärtusi omistada: normaaljaotuse defineerivad mean ja sd, t jaotuse aga df (vabadusastmete arv = n -1) ja ncp (non-centrality parameter).** 
  
```{r}
set.seed(19)
df <- tibble(a=rnorm(3)) 
df %>% ggplot(aes(x=a)) +
  geom_dotplot() + 
  stat_function(fun=dnorm, args=list(mean=mean(df$a), 
                   sd=sd(df$a)), colour="red") + 
  stat_function(fun=dnorm, args=list(mean=0, 
                   sd=1), colour="blue") +
  stat_function(fun=dt, args=list(df=2, ncp=mean(df$a))) +
  xlim(-3, 3) + ylim(0, 0.55) + labs(x=NULL, y=NULL) +
  theme_tufte()
```
*Joonis: juhuvalim normaaljaotusest, mille keskmine=0 ja standardhälve=1 (n=3, andmepunktid näidatud mustade punktidega). Sinine joon - normaaljaotus, millest tõmmati valim; punane joon - normaaljaotuse mudel, mis on fititud valimi andmetel; must joon - Studenti t jaotuse mudel, mis on fititud samade andmetega.*


```{r}
mean(df$a); sd(df$a)

aa <- rnorm(1000, mean(df$a), sd(df$a))
mean(aa); sd(aa)
```
Nagu näha on uute modelleeritud (simuleeritud) andmete keskväärtus ja SD väga sarnased algsete andmete omale, mida kasutasime mudeli fittimisel. Kahjuks on aga meie mudel üle-fittitud, mis tähendab, et ta kajastab liigselt neid valimi aspekte, mis ei peegelda algse populatsiooni omadusi.

```{r}
a <- rt(1000, df=2, ncp=mean(df$a))
sd(a)
```

Siin on meil 2 mudelit, normaaljaotus punaselt ja studenti t jaotus mustalt, mis mõlemad sobituvad hästi andmetega. Mõlemad mudelid alahindavad keskmist (tegelik keskmine = 0).  Lisaks: normaaljaptuse mudel alahindab varieeruvust ja t jaotuse mudel ülehindab varieeruvust (tegelik varieeruvus = 1). Loomulikult ei vasta ükski mudel päriselt tegelikkusele. Küsimus on pigem selles, kas mõni meie mudelitest on piisavalt hea, et olla kasulik. Vastus sellele sõltub, milleks plaanime oma mudelit kasutada.

Selline viis teadust teha eeldab, et me fitime samade empiiriliste andmetega mitu erinevat mudelit, valime välja ühe mudeli (või mudelite perekonna), mis andmetega kõige paremini sobib, ning püüame seda mudelit teadusliku hüpoteesiga sobitada. Jällegi, me võime kindlad olla, et me ei ole andmete vastu testinud kõiki võimalikke mudeleid (mida on lõpmatu hulk), mistõttu ei ole meil kunagi põhjust arvata, et me oleme oma töö tulemusel isoleerinud päriselt parima mudeli. Mudelite formuleerimine, fittimine ja omavahel võrdlemine on osa statistikast, samas kui võitja-mudeli võrdlemine teadusliku hüpoteesiga on mitte-matemaatiline (või rohkem kui matemaatiline) ja kuulub teaduse valda. 

####normaaljaotuse ja lognormaaljaotuse erilisus

Normaaljaotus ja lognormaaljaotus on erilised sest 

(1) keskne piirteoreem ütleb, et olgu teie valim ükskõik millise jaotusega, paljudest valimitest arvutatud **aritmeetilised keskmised** on alati enam-vähem normaaljaotusega (kui n>30). See viib "elementaarsete vigade hüpoteesile", mille kohaselt paljude väikeste üksteisest sõltumatute juhuslike efektide (vigade) summa annab tulemuseks normaaljaotuse. Regressioonimudelid eeldavad andmete (vigade) normaaljaotust.   
Paraku annavad enamus mõõtmisi, mida me teeme, eranditult mitte-negatiivseid tulemusi. Sageli on sellised jaotused ebasümmeetrilised (v.a., kui cv = sd/mean on väike). Siis on meil sageli tegu lognormaaljaotusega, mis tekkib log-normaalsete muutujate korrutamisest (mitte liitmisest, nagu normaaljaotuse puhul). Keskne piirteoreem 2: suvalise jaotusega muutujate **geomeetrilised keskmised** on lognormaaljaotusega. "Elementaarsete vigade hüpotees 2": Kui juhuslik varieeruvus tekib paljude juhuslike efektide korrutamisel, on tulemuseks lognormaaljaotus.  Lognormaaljaotuse elementide (arvude) logaritmimisel saame normaaljaotuse. 

(2) Mõlemad jaotused (normaal ja lognormaal) on maksimaalse entroopiaga jaotused. See tähendab, et väljaspool oma parameetrite tuunitud väärtusi on nad minimaalselt informatiivsed. Näiteks, andes normaaljaotusele ette keskväärtuse ja standardhälbe lisame me sinna garanteeritult minimaalselt muud (sooviamtut) informatsiooni. Teised jaotused, mis sellele kriteeriumile vastavad, on eksponentsiaalne jaotus, binoomjaotus ja poissoni jaotus. Maksimaalse entroopiaga jaotused sobivad hästi Bayesi prioriteks sest me suudame paremini kontrollida, millist informatsiooni me neisse suuname (me ei lisa sinna kogemata soovimatut infot). 

>  teadlane töötab korraga kolmes maailmas: 1. päris maailm, 2. teadusliku teooria maailm ja 3. statistilise mudeli maailm. Kõik statistilised vastused sõltuvad andmete kvaliteedist, nende juhuslikust varieeruvusest ning kasutatud mudelitest. 

##1.2. Küsimused

Statistika abil saab vastuseid järgmisetele küsimustele:

1)  kuidas näevad välja teie andmed: milline on just teie andmete jaotus, keskväärtus, varieeruvus ja koos-varieeruvus? Näiteks, mõõdetud pikkuste ja kaalude koos-varieeruvust saab mõõta korrelatsioonikordaja abil.
2)  mida me peaksime teie andmete põhjal uskuma parameetri tegeliku väärtuse kohta? Näiteks, kui meie andmete põhjal arvutatud keskmine pikkus on 178 cm, siis kui palju on meil põhjust arvata, et tegelik populatsiooni keskmine pikkus > 185 cm?
3)  mida ütleb statistilise mudeli struktuur teadusliku hüpoteesi kohta? Näiteks, kui meie poolt mõõdetud pikkuste ja kaalude koos-varieeruvust saab hästi kirjeldada kindlat tüüpi lineaarse regressioonimudeliga, siis on meil ehk tõendusmaterjali, et pikkus ja kaal on omavahel sellisel viisil seotud ja eelistatud peaks olema teaduslik teooria, mis just sellise seose tekkimisele bioloogilise mehhanismi annab.
4) mida ennustab meie mudel tuleviku kohta? Näiteks, meie lineaarne pikkuse-kaalu mudel suudab ennustada tulevikus kogutavaid andmeid. Aga kui hästi?

> statistika ülesanne on lähtuvalt piiratud hulgast andmetest ja mudelitest kvantifitseerida parimal võimalikul viisil kõhedust, mida peaksime tundma vastates eeltoodud küsimustele.

Statistika ei vasta otse teaduslikele küsimustele ega küsimustele päris maailma kohta. Statistilised vastused jäävad alati kasutatud andmete ja mudelite piiridesse. Kahjuks.

#2. Kuidas näevad välja teie andmed?
##2.1. summaarsed statistikud

Summaarne statistik = üks number.  
Milliseid statistikuid arvutada ja milliseid vältida, sõltub statistilisest mudelist 

> summaarse statistika abil iseloomustame a) tüüpilist valimi liiget (keskmist), b) muutuja sisest varieeruvust, c) erinevate muutujate (pikkus, kaal vms) koos-varieeruvust

###2.1.1. keskväärtused

Kui valim on normaaljaotusega (histogramm on sümmeetriline), hinda tüüpilist liiget läbi aritmeetilise keskmise (mean).  Muidu kasuta mediaani (median). Kui valim on liiga väike, et jaotust hinnata (aga > 4), eelista mediaani. Mediaani saamiseks järjestatakse mõõdetud väärtused suuruse järgi ja võetakse selle rea keskmine liige. Mediaan on vähem tundlik ekstreemsete väärtuste (outlierite) suhtes kui mean. Kolmas võimalus määrata tüüpilist liiget on mood (mode) – jaotuse tipp. Seda on aga raskem täpselt määrata ja mitmetipulisel jaotusel on mitu moodi. Leidub valimeid, mille tüüpilist liiget ei ole põhimõtteliselt võimalik ühe numbriga iseloomustada – sest lihtsalt ei leidu ühte sorti tüüpilist liiget.

```{r}
andmed <- rlnorm(100)
hist(andmed, breaks = 20)
mean(andmed)
median(andmed)

mode <-  function(x, adjust=1) {
  x <- na.omit(x)
  dx <- density(x, adjust=adjust)
y_max <- dx$x[which.max(dx$y)] 
y_max
}
mode(andmed)

plot(density(andmed, adjust=1))
abline(v=mode(andmed), col="red")
abline(v=median(andmed), col="blue")
abline(v=mean(andmed))

```


###2.1.2. muutuja sisene varieeruvus

Mean-iga käib kokku standardhälve (SD). 
```{r}
sd(andmed)
```
SD on sama ühikuga, mis andmed (ja andmete keskväärtus). Statistikute hulgas eelistatud esitus on mean (SD), mitte mean (+/- SD). 1 SD katab 68% normaaljaotusest, 2 SD – 96% ja 3 SD – 99%. Normaaljaotus langeb servades kiiresti, mis tähendab, et tal on peenikesed sabad ja näiteks 5 SD kaugusel keskmisest paikneb vaid üks punkt miljonist. Näiteks: inimeste IQ on normaaljaotusega, mean=100, SD=15. See tähendab, et kui sinu IQ=115 (ülikooli astujate keskmine IQ), siis on tõenäosus, et juhuslikult kohatud inimene on sinust nutikam, 18% ((100% - 68%)/2  = 18%). 
Kui aga “tegelikul” andmejaotusel on “paks saba” või esinevad outlierid, siis normaaljaotust eeldav mudel tagab ülehinnatud SD ja seega ülehinnatud varieeruvuse. Kui andmed saavad olla ainult positiivsed, siis SD > mean/2 viitab, et andmed ei sobi normaaljaotuse mudeliga (sest mudel ennustab negatiivsete andmete esinemist küllalt suure sagedusega). 
Kui andmed ei sobi normaaljaotusesse siis võib mean (SD) asemel pakkuda kahte alternatiivset võimalust: 

####2.1.2.1	logaritmi andmed. 

Kui logaritmimine muudab andmed normaalseks, siis saab logaritmitud andmetest arvutada mean-i ja SD ja seejärel mõlemad anti-logaritmida. Sellisel juhul avaldad sa lõpuks geomeetrilise keskmise ja multiplikatiivse SD (multiplikatiivne SD = geom mean x SD; geom mean/SD). Geomeetriline keskmine on alati väiksem kui aritmeetiline keskmine. Lisaks on SD interval nüüd asümmeetriline ja SD on alati > 0. Nagu ennegi, 68% lognormaalsetest andmetest jääb 1SD vahemikku ning 95.5% andmetest jääb 2SD vahemikku.

lognormaalsete andmete tavapärane iseloomustus keskmise ja standardhälbega: mean(sd) on 1.8(1.9). See sd interval on sümmeetriline, ehkki andmete jaotus on vägagi ebasümmeetriline. Lisaks ennustab standardhälve, mis on suurem kui keskväärtus, suure sagedusega negatiivseid väärtusi. Sageli on aga negatiivsed muutuja väärtused võimatud (näiteks nädalas suitsetatud sigarettide arv). See on näide halvast mudelist!

Juhul kui tegu lognormaalsete andmetega on meil võimalus kasutada palju paremat mudelit varieeruvusele - multiplikatiivset standardhälvet:

```{r}

multiplicative_sd <- function(x) {
  x <- na.omit(x)
  a <- log10(x)
  b <- mean(a)
  c <- sd(a)
  geom_mean <- 10**b
  mult_sd <- 10**c
  lower1 <- geom_mean/mult_sd
  upper1 <- geom_mean * mult_sd
  lower2 <- geom_mean/(mult_sd**2)
  upper2 <- geom_mean * (mult_sd**2)
  Mean <- mean(x)
  lower3 <- mean(x) - sd(x)
  upper3 <- mean(x) + sd(x)
  lower4 <- mean(x) - sd(x)*2
  upper4 <- mean(x) + sd(x)*2
  results <- tibble(SD=c("multiplicative_SD", 
                         "multiplicative_2SD", 
                         "additive_SD", 
                         "additive_2SD"), 
                    MEAN=c(geom_mean, 
                           geom_mean, 
                           Mean, 
                           Mean), 
                    lower=c(lower1, 
                            lower2, 
                            lower3, 
                            lower4), 
                    upper=c(upper1, 
                            upper2, 
                            upper3, 
                            upper4) )
  results
}

multiplicative_sd(andmed)
```

Pane tähele, et tavalise aritmeetitilise keskmise asemel on meil nüüd geomeetriline keskmine.  Võrdluseks on antud ka tavaline (aritmeetiline) keskmine ja (aditiivne) SD. Additiivne SD on selle jaotuse kirjeldamiseks selgelt ebaadekvaatne (vt jaotuse pilti ülalpool ja võrdle mulitplikatiivse SD-ga).


Kuidas aga töötab multiplikatiivne standardhälve normaaljaotusest pärit andmetega? Kui normaalsete andmete peal multiplikatiivse sd rakendamine viib katastroofini, siis pole sel statistikul suurt praktilist kasutusruumi.
```{r}
set.seed(5363)
norm_andmed <- rnorm(3, 100, 20)
multiplicative_sd(norm_andmed)
```

Nagu näha, on multiplikatiivse sd kasutamine normaalsete andmetega üsna ohutu (kuigi, ainult niikaua, kuni meil puuduvad negatiivsed andmed). Seega, kui sa ei ole kindel, kas tegu on normaaljaotusega või lognormaaljaotusega, arvesta, et lognormaaljaotus on bioloogias üsna tavaline (eriti ensüümreaktsioonide ja kasvuprotsesside juures). Seega on mõistlik alati kasutada multiplicative_sd() funktsiooni ja kui mõlema SD väärtused on sarnased, siis võib loota, et andmed on normaalsed ning saab avaldada tavapärase additiivse SD refereede rõõmuks.

> kui n< 10, siis mõlemad SD-d alahindavad tehnilistel põhjustel tegelikku sd-d --- olge ettevaatlik väikeste valimitega!

NB! SIIN ON KALA - MARGUNNILE!!! Multiplikatiivne sd kasutab siiski oma arvutuses tavalist sd-d (vt funktsiooni). Sellel on paraku oluline puudus: väikse valimi korral (n<10) alahindab see tegelikku varieeruvust. Liiga väike sd tähendab aga, et teadlased, kes töötavad väikeste valimitega on sageli liiga optimistlikud oma võime suhtes leida oma andmetest tõelisi efekte. Sest päris efekt tuleb statistika abil "välja prepareerida" mürast, mis tuleneb sellest, et ükski juhuvalim ei peegelda täpselt populatsiooni, millest see pärineb. Ja mida väiksem valim, seda suurem on sellest tulenev juhuslik müra.

```{r}

pop.sd <- function(x) {
  n <- length(x)
  sd(x) * sqrt( (n - 1) / n)
}
set.seed(34342)
andmed1 <- rep(1:1000, each=3)
df <- tibble(andmed=rnorm(3000, 100, 15), indeks=as.factor(andmed1))
df1 <- df %>% group_by(indeks) %>% summarise(pop_sd=pop.sd(andmed), SD=sd(andmed))
df1 %>% ggplot(aes(pop_sd)) + geom_density(fill="red", alpha=0.5) + geom_density(aes(SD), fill="blue",alpha=0.5) + xlim(-10, 40)

```

####2.1.2.2	iseloomusta andmeid algses skaalas: median (MAD). 

MAD –-- median absolute deviation --- on vähem tundlik outlierite suhtes ja ei eelda normaaljaotust. Puuduseks on, et MAD ei oma tõlgendust, mille kohaselt ta hõlmaks kindlat protsenti populatsiooni või valimi andmejaotusest.

```{r}
mad(andmed, constant = 1)
```


> Ära kunagi avalda andmeid vormis: mean (MAD) või median (SD). 
    Korrektne on mean(SD) või median(MAD).

###2.1.3. muutujate koos-varieeruvus

Andmete koos-varieeruvust mõõdetakse korrelatsiooni abil. Tulemuseks on üks number - korrelatsioonikordaja r, mis varieerub -1 ja 1 vahel. 

    r = 0 – kahte tüüpi mõõtmised (x=pikkus, y=kaal) samadest mõõteobjektidest varieeruvad üksteisest sõltumatult. 
    r = 1: kui ühe muutuja väärtus kasvab, kasvab ka teise muutuja väärtus alati täpselt samas proportsioonis. 
    r = -1: kui ühe muutuja väärtus kasvab, kahaneb teise muutuja väärtus alati täpselt samas proportsioonis. 
    
Kui r on -1 või 1, saame me x väärtust teades täpselt ennustada y väärtuse (ja vastupidi, teades y väärrust saame täpselt ennustada x väärtuse).    
Kuidas tõlgendame aga tulemust r = 0.9? Mitte kuidagi. Selle asemel tõlgendame r2 = 0.92 = 0.81 – mis tähendab, et x-i varieeruvus suudab seletada 81% y varieeruvusest ja vastupidi, et Y-i varieeruvus suudab seletada 81% X-i varieeruvusest. 

```{r}
cor(iris$Sepal.Length, iris$Sepal.Width, use="complete.obs") 

correlation<-cor.test(iris$Sepal.Length, iris$Sepal.Width, na.rm=T, method = "pearson") # a list of 9

names(correlation)
#str(correlation)
#correlation$conf.int
```


Korrelatsioonikordaja väärtus sõltub mitte ainult andmete koos-varieeruvusest vaid ka andmete ulatusest. Suurema ulatusega andmed X ja/või Y teljel annavad keskeltläbi 0-st kaugemal oleva korrelatsioonikordaja. Selle pärast sobib korrelatsioon halvasti näiteks korduskatsete kooskõla mõõtmiseks. 

Lisaks, korrelatsioonikordaja mõõdab vaid andmete *lineaarset* koos-varieeruvust: kui andmed koos-varieeruvad mitte-lineaarselt, siis võivad ka väga tugevad koos-varieeruvused jääda märkamatuks.

> Kõik summaarsed statistikud kaotavad enamuse teie andmetes leiduvast infost – see kaotus on õigustatud ainult siis, kui teie poolt valitud statistik iseloomustab hästi andmete sügavamat olemust (näiteks tüüpilist mõõtmistulemust või andmete varieeruvust).


psych package corr.test gives a correlation matrix and p values adjusted for multiple testing.

```{r}
#numeric columns only!
print(psych::corr.test(iris[-5], use="complete"), short = FALSE)
```



##2.2 EDA --- eksploratoorne andmeanalüüs

Kui ühenumbriline andmete summeerimine täidab eelkõige kokkuvõtliku kommunikatsiooni eesmärki, siis EDA on suunatud teadlasele endale. EDA eesmärk on andmeid eelkõige graafiliselt vaadata, et saada aimu 1) andmete kvaliteedist ja 2) lasta andmetel "sellisena nagu nad on" kõneleda ja sugereerida uudseid teaduslikke hüpoteese. Neid hüpoteese peaks siis testima formaalse statistilise analüüsi abil.

Kõigepealt vaatame andmeid numbrilise kokkuvõttena:
```{r}
psych::describe(iris)

```

Millised korrelatsioonid võiksid andmetes esineda?
```{r}
library(corrgram) #PCA for ordering

corrgram(iris, order=TRUE, 
         lower.panel = panel.pts,
         upper.panel = panel.ellipse,
         diag.panel = panel.density,
         main="Correlogram of diamond dataset")
```

###2.2.1 Kuidas uurida muutuja sisest varieeruvust

Muutuja - midagi, mida mõõdeti (näiteks mõõteobjektide kaal). 
Kui iga muutuja kohta on vaid üks number, mida plottida, kasuta Cleveland plotti:

```{r}
dd <- diamonds %>% group_by(clarity) %>% summarise(number_of_diamonds=n())
dd %>% ggplot(aes(x=number_of_diamonds, 
                  y=reorder(clarity, number_of_diamonds))) +
  geom_point(size=3) +
  theme_bw() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_line(colour="grey60", linetype="dashed")) +
  labs(y="clarity")
```


1) N < 20 - ploti iga andmepunkt eraldi (stripchart(), plot()) ja keskmine või mediaan. 
2) 20 > N > 100: geom_dotplot()
3) N > 100: geom_histogram(), geom_density() --- nende abil saab ka 2 kuni 6 jaotust võrrelda
4) Mitme jaotuse kõrvuti vaatamiseks kui N > 15: geom_boxplot() or, when N > 50, geom_violin(), geom_joy() 

If N is small (less than 20, say), it makes more sense to show individual data points and mean: 

```{r}
library(ggthemes)
df2 <- tibble(pop = rep(c("male", "female"), each = 1000), 
              value = c(rnorm(1000, mean = 1), rnorm(1000, mean = 1.8)))
p <- df2 %>% sample_n(20) 

p %>% ggplot(aes(pop, value)) + geom_jitter(width = 0.1) + 
  stat_summary(fun.y = median, geom = "point", shape = 95, 
               color = "red", size = 15) +
  theme_tufte()
```

Dotplot:
```{r}
ggplot(iris, aes(Sepal.Length)) + geom_dotplot()
```

Histogram:

```{r}
ggplot(iris, aes(Sepal.Length)) + 
  geom_histogram(bins = 10, color="white", fill = "navyblue") 
```

```{r}
library(ggthemes)
d <- iris        # Full data set
d_bg <- d[, -5]  # Background Data - full without the 5th column (Species)

ggplot(data = d, aes(x = Sepal.Width, fill = Species)) +
  geom_histogram(data = d_bg, fill = "grey", alpha=0.8, bins=10) +
  geom_histogram(colour = "black", bins=10) +
  facet_grid(Species~.) +
  guides(fill = FALSE) +  # to remove the legend
  theme_tufte()          # for clean look overall

```

density plot:
```{r}
iris%>%ggplot()+
  geom_density(aes(Sepal.Width, fill=Species, color=Species, alpha=0.5))+
  theme_tufte()

```

joyplot

```{r}
library(ggjoy)
ggplot(iris, aes(x=Sepal.Length, y=Species, fill=Species)) + 
  geom_joy(scale=4, rel_min_height=0.01, alpha=0.9) +
  theme_joy(font_size = 13, grid=TRUE) + 
  theme(legend.position = "none")
```

Boxplot:
```{r}
ggplot(iris, aes(Species, Sepal.Width, fill=Species)) + geom_boxplot()
```

violin plot plus jitterplot:
```{r}
ggplot(iris, aes(Species, Sepal.Width)) + 
  geom_violin(aes(fill=Species)) +
  geom_jitter(width = 0.1, alpha=0.4, size=0.5)
```


###2.2.2 Kahe muutuja koos-varieerumine 

```{r}
  
ggplot(data = diamonds, aes(x = depth, y = price)) +
  geom_point(size=0.1, alpha=0.1) +
  geom_density2d()
```


Fit a linear model and plot the dots and model:
```{r}
ggplot(data=iris, aes(Sepal.Width, Petal.Width))+
  geom_point()+
  geom_smooth(method="lm", color="red") 
```

####Kokkuvõte:

a.	Andmepunktide plottimine säilitab maksimaalselt andmetes olevat infot (nii kasulikku infot kui müra). Aitab leida outliereid (valesti sisestatud andmeid, valesti mõõdetud proove jms). Kui valim on väiksem kui 20, piisab täiesti üksikute andmepunktide plotist koos mediaaniga. Dot-plot – 1D, 2D, 3D
b.	Histogramm – kõigepealt mõõtskaala ja seejärel andmed jagatakse võrdse laiusega binnidesse ja plotitakse binnide kõrgused. Bin, kuhu läks 20 andmepunkti on 2X kõrgem kui bin, kuhu läks 10 andmepunkti. Samas, bini laius/ulatus mõõteskaalal pole teile ette antud – ja sellest võib sõltuda histogrammi kuju. Seega on soovitav proovida erinevaid bini laiusi ja võrrelda saadud histogramme. Histogramm sisaldab vähem infot kui dot plot, aga võimaldab paremini tabada seaduspärasid & andmejaotust & outliereid suurte andmekoguste korral.
c.	Box-plot – sisaldab vähem infot kui histogramm, kuid neid on lihtsam kõrvuti võrrelda. Levinuim variant (kuid kahjuks mitte ainus) on Tukey box-plot – mediaan (joon), 50% IQR (box) ja 1,5x IQR (vuntsid), pluss outlierid eraldi punktidena. 
d.	Violin plot – informatiivsuselt box-ploti ja histogrammi vahepeal – sobib paljude jaotuste kõrvuti võrdlemiseks
e.	Line plot – kasuta ainult siis kui nii X kui Y teljele on kantud pidev väärtus (pikkus, kaal, kontsentratsioon, aeg jms). Ära kasuta, kui teljele kantud punktide vahel ei ole looduses mõtet omavaid pidevaid väärtusi (näiteks X teljel on katse ja kontroll või erinevad valgumutatsioonid, mille aktiivsust on mõõdetud) 
f.	Suhete võrdlemine (pie vs bar)
g.	Cleveland plot countide jaoks. Kasuta Barplotti ainult siis, kui Cleveland plot vm plot mingil põhjusel ei sobi. Barplot võiks olla viimane valik.

> Informatsiooni hulk kahanevalt: 
iga andmepunkt plotitud (dot plot) -> 
histogram -> 
density plot/violin plot -> 
box plot -> 
bar plot standardhälvetega -> 
Cleveland plot (barplot ilma veapiirideta) 

##Jäta meelde:

1. statistika uurib formaalseid mudeleid, mitte teooriaid ega päris maailma.
2. Statistika jagatakse kahte ossa: kirjeldav ja järeldav (inferential). 
3. Kirjeldav statistika kirjeldab teie andmeid summaarsete statistikute ning graafiliste meetodite abil. 
4. Järeldav statistika püüab teie andmete põhjal teha järeldusi statistilise populatsiooni kohta, millest need andmed pärinevad
5. Statistika põhiline ülesanne on kvantifitseerida ebakindlust, mis ümbritseb neid järeldusi.

##Sõnastik

* Statistiline populatsioon – objektide kogum, millele soovime teha statistilist üldistust. Näiteks hinnata keskmist ravimi mõju patsiendipopulatsioonis. Või Escherichia coli ensüümi X keskmist Kcat-i. 

* Valim – need objektid (patsiendid, ensüümiprepid), mida me reaalselt mõõdame. 

* Juhuvalim – valim, mille liikmed on populatsioonist valitud juhuslikult ja iseseisvalt. See tähendab, et kõigil populatsiooni liikmetel (kõikidel patsientidel või kõikidel võimalikel ensüümipreparatsioonidel) on võrdne võimalus sattuda valimisse JA, et valimisse juba sattunud liikme(te) põhjal ei ole võimalik ennustada järgmisena valimisse sattuvat liiget. 

* Esinduslik valim – Valim on esinduslik, kui ta peegeldab hästi statistilist populatsiooni. Ka juhuvalim ei pruugi olla esinduslik (juhuslikult).

* Statistik – midagi, mis on täpselt arvutatud valimi põhjal (näiteks pikkuste keskmine)

* Parameetri väärtus – teadmata suurus, mille täpset väärtust me saame ainult umbkaudu ennustada aga mitte kunagi täpselt teada. (näiteks mudeli intercept, populatsiooni keskmine pikkus; efekti suurus = katsegrupi keskmine – kontrollgrupi keskmine)

* Statistiline mudel – matemaatiline formaliseering, mis sageli koosneb 2st osast: determinismlik protsessi-mudel pluss juhuslik vea/varieeruvuse-mudel. 
Protsessi-mudeli näiteks kujutle, et mõõdad mitme inimese pikkust (X muutuja) ja kaalu (Y muutuja). Sirge võrrandiga Y = a + b * X (kaal = a + b * pikkus) saab anda determinismliku lineaarse ennustuse kaalu kohta: kui X (pikkus) muutub ühe ühiku (cm) võrra, siis muutub Y (kaal) väärtus keskmiselt b ühiku (kg) võrra. Seevastu varieeruvuse-mudel on tõenäosusjaotus (näit normaaljaotus). Selle abil modelleeritakse Y-suunalist andmete varieeruvust igal X väärtusel (näiteks, milline on 182 cm pikkuste inimeste oodatav kaalujaotus).  Mudel on seega tõenäosuslik: me saame näiteks küsida: millise tõenäosusega kaalub 182 cm pikkune inimene üle 100 kilo. Mida laiem on varieeruvuse mudeli Y-i suunaline jaotus igal X-i väärtusel, seda kehvemini ennustab mudel, millist Y väärtust võime konkreetselt oodata mingi X-i väärtuse korral. Lineaarsete mudelite eesmärk ei ole siiski mitte niivõrd uute andmete ennustamine (seda teevad paremini keerulised mudelid), vaid mudeli struktuurist lähtuvalt põhjuslike hüpoteeside püstitamine/kontrollimine (kas inimese pikkus võiks otseselt reguleerida/kontrollida tema kaalu?). Kuna selline viis teadust teha töötab üksnes lihtsate mudelite korral, on enamkasutatud statistilised mudelid taotluslikult lihtsustavad ja ei pretendeeri tõelähedusele. 

* Tehniline replikatsioon – sama proovi (patsienti, ensüümipreparatsiooni, hiire pesakonna liiget) mõõdetakse mitu korda. Mõõdab tehnilist varieeruvust ehk mõõtmisviga. Seda püüame kontrollida parandades mõõtmisaparatuuri/protokolle või siis juba andmete tasemel, statistilise analüüsiga. Näiteks saame andmeid agregeerida ja arvutada keskväärtuse. Kui andmepunkte on piisavalt ja varieeruvus on sümmeetriline ümber tõelise populatsiooniväärtuse, siis annab keksväärtus meile hea hinnangu parameetri tõelisele väärtusele.

* Bioloogiline replikatsioon – erinevaid patsiente, ensüümipreppe, erinevate hiirepesakondade liikmeid mõõdetakse, igaüht üks kord. Eesmärk on mõõta Bioloogilist varieeruvust, mis tuleneb mõõteobjektide reaalsetest erinevustest: iga patsient ja iga ensüümimolekul on erinev kõigist teistest omasugustest. Bioloogiline varieeruvus on teaduslikult huvitav ja seda saab visualiseerida algandmete tasemel (mitte keskväärtuse tasemel) näiteks histogrammina.
Teaduslikke järeldusi tehakse bioloogiliste replikaatide põhjal. Tehnilised replikaadid seevastu kalibreerivad mõõtesüsteemi täpsust. Kui te uurite soolekepikest E. coli, ei saa te teha formaalset järeldust kõigi bakterite kohta. Samamoodi, kui te uurite vaid ühe hiirepesakonna/puuri liikmeid, ei saa te teha järeldusi kõikide hiirte kohta. Kui teie katseskeem sisaldab nii tehnilisi kui bioloogilisi replikaate on lihtsaim viis neid andmeid analüüsida kõigepealt keskmistada üle tehniliste replikaatide ning seejärel kasutada saadud keskmisi edasistes arvutustes üle bioloogiliste replikaatide (näiteks arvutada nende pealt uue keskmise, standardhälve ja/või usaldusintervalli). Selline kahe-etapiline arvutuskäik ei ole siiski optimaalne. Optimaalne, kuid keerukam, on panna mõlemat tüüpi andmed ühte hierarhilisse mudelisse.

##Tõenäosuse (P) reeglid on ühised kogu statistikale: 
* P jääb 0 ja 1 vahele; P(A) = 1 tähendab, et sündmus A toimub kindlasti. 
* kui sündmused A ja B on üksteist välistavad, siis tõenäosus, et toimub sündmus A või sündmus B on nende kahe sündmuse tõenäosuste summa --- P(A v B) = P(A) + P(B). 
* Kui A ja B ei ole üksteist välistavad, siis P(A v B) = P(A) + P(B) – P(A & B).
* kui A ja B on üksteisest sõltumatud (A toimumise järgi ei saa ennustada B toimumist ja vastupidi) siis tõenäosus, et toimuvad mõlemad sündmused on nende sündmuste tõenäosuste korrutis –-- P(A & B) = P(A) x P(B). 
* Kui B on loogiliselt A alamosa, siis P(B) < P(A)
* P(A | B) –-- tinglik tõenäosus. Sündmuse A tõenäosus, tingimusel et sündmus B on juba toimunud. P(vihm | pilves ilm) ei ole sama, mis P(pilves ilm | vihm).
* Juhul kui P(B)>0, siis P(A | B) = P(A & B)/P(B) ehk
* P(A | B) = P(A) x P(B | A)/P(B) –-- Bayesi teoreem.
* Kui P(A) = 1 või 0, siis ei leidu viisi, kuidas uute andmete lisandumine saaks seda tõenäosust mõjutada (see tuleneb Bayesi teoreemist).  

Kuigi kõik statistikud lähtuvad tõenäosustega töötamisel täpselt samadest matemaatilistest reeglitest, tõlgendavad erinevad koolkonnad saadud numbreid erinevalt. Kaks põhilist koolkonda on sageduslikud statistikud ja Bayesiaanid. 

* Tõenäosus, sageduslik tõlgendus – pikaajaline sündmuste suhteline sagedus. Näiteks 6-te sagedus paljudel täringuvisetel. Sageduslik tõenäosus on teatud tüüpi andmete sagedus, tingimusel et nullhüpotees (H0) kehtib; ehk P(teatud tüüpi andmed | H0). Nullhüpotees ütleb enamasti, et uuritava parameetri (näiteks ravimiefekti suurus) väärtus on null. Seega, kui P on väike, ei ole seda tüüpi andmed kooskõlas arvamusega, et parameetri väärtus on null (mis aga ei tähenda automaatselt, et sa peaksid uskuma, et parameetri väärtus ei ole null). 

* Tõenäosus, Bayesi tõlgendus – usu määr mingisse hüpoteesi. Näiteks 62% tõenäosus (et populatsiooni keskmine pikkus < 180 cm) tähendab, et sa oled ratsionaalse olendina nõus kulutama mitte rohkem kui 62 senti kihlveo peale, mis võidu korral toob sulle sisse 1 EUR. Bayesi tõenäosus omistatakse statistilisele hüpoteesile (näiteks, et ravimiefekti suurus jääb vahemikku a kuni b), tingimusel, et sul on täpselt need andmed, mis sul on; ehk P(hüpotees | sinu andmed). Seega, kui P on väike, on sul põhjust uskuda, et sind päriselt huvitav hüpotees ei kehti --- aga ainult niipalju kui sa usud kasutatud mudelitesse.





